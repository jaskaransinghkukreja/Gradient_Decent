Gradient Decent 

This project demonstrates how Linear Regression can be implemented using Gradient Descent from scratch in Python, without relying on built-in optimization functions. It helps in understanding how model parameters (slope and intercept) are updated iteratively to minimize the error.

The class Gradient_Decent_Scratch represents a custom implementation of Gradient Descent for a simple linear regression model:

   y_cap= m * X + b

where

m is the slope (coefficient),

b is the intercept,

lr is the learning rate,

epochs is the number of iterations.
